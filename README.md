This repository contains the notebooks and files I used to tackle Kaggle's Titanic-competition
The goal of the competition is to develop a machine learning model(s) that predict the survival of Titanic passengers as accurately as possible.

The competition: https://www.kaggle.com/c/titanic
My Kaggle profile: https://www.kaggle.com/jubakala


The list of machine learning models used and reached accuracy for each model:
- XGBoost (0.758)
- Random Forest Classifier (0.763)
- Support Vector Machine (0.77)
- Kernel SVM (0.7799)
- Logistic Regression (0.775)
- K-Nearest Neighbors (0.708)
- Naive Bayes (0.751)
- Decision Tree (0.693)

- Ensemble 01: Taking the "majority vote" of all the above models (0.784)
- Ensemble 02: Taking the "majority vote" of three best models above (0.777)
